{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10084450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a058739e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2532214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from face_alignment import align\n",
    "from backbones import get_model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "770d7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46044f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajni\\AppData\\Local\\Temp\\ipykernel_16644\\3159899112.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location='cuda'))\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_name=\"edgeface_xs_gamma_06\" # or edgeface_xs_gamma_06\n",
    "model=get_model(model_name)\n",
    "checkpoint_path=f'checkpoints/{model_name}.pt'\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location='cuda'))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7768130",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "38615ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_face_pairs(file_path):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                # Same person, positive pair (1)\n",
    "                person, idx1, idx2 = parts\n",
    "                idx1 = int(idx1)\n",
    "                idx2 = int(idx2)\n",
    "                pairs.append((f\"{person}_{idx1:04d}\", f\"{person}_{idx2:04d}\"))\n",
    "                labels.append(1)\n",
    "            elif len(parts) == 4:\n",
    "                # Different people, negative pair (0)\n",
    "                person1, idx1, person2, idx2 = parts\n",
    "                idx1 = int(idx1)\n",
    "                idx2 = int(idx2)\n",
    "                pairs.append((f\"{person1}_{idx1:04d}\", f\"{person2}_{idx2:04d}\"))\n",
    "                labels.append(0)\n",
    "    \n",
    "    return pairs, labels\n",
    "\n",
    "# Function to load an image given the person and index\n",
    "def load_image(person, idx, lfw_funneled_dir):\n",
    "    image_path = os.path.join(lfw_funneled_dir, person, f\"{person}_{int(idx):04d}.jpg\")\n",
    "    if os.path.exists(image_path):\n",
    "        # Return the path if the image exists\n",
    "        return image_path\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5a330429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image path into an actual image and preprocess it\n",
    "def get_embeddings(image_path):\n",
    "    try:\n",
    "        # Load the image using PIL (or OpenCV)\n",
    "     \n",
    "        aligned = align.get_aligned_face(image_path)  # Assuming `align.get_aligned_face` works with PIL image\n",
    "        transformed_input = transform(aligned)  # Apply the necessary transformations (e.g., normalization)\n",
    "        transformed_input = transformed_input.unsqueeze(0)  # Add batch dimension\n",
    "        transformed_input = transformed_input.to(device)  \n",
    "        embedding = model(transformed_input)  # Get the embedding from your model\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None  # Return None if there's any issue\n",
    "\n",
    "# Calculate cosine similarity using PyTorch's built-in function\n",
    "def get_similarity(embedding1, embedding2):\n",
    "    # Ensure embeddings are on CPU for cosine similarity\n",
    "    embedding1 = embedding1.cpu()\n",
    "    embedding2 = embedding2.cpu()\n",
    "    \n",
    "    # Normalize the embeddings (unit vectors) before calculating cosine similarity\n",
    "    similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "    return similarity.item()\n",
    "\n",
    "# Function to evaluate the LFW dataset using pairs\n",
    "def evaluate_lfw(pair_file_path, lfw_funneled_dir, device, threshold=0.5):\n",
    "    pairs, labels = read_face_pairs(pair_file_path)\n",
    "    predictions = []\n",
    "    skipped = 0  # Count how many pairs are skipped\n",
    "    \n",
    "    for (image1, image2), label in zip(pairs, labels):\n",
    "        # Extract person names and indices from image identifiers\n",
    "        person1, idx1 = image1.rsplit('_', 1)\n",
    "        person2, idx2 = image2.rsplit('_', 1)\n",
    "        \n",
    "        try:\n",
    "            # Load and generate embeddings for the two images\n",
    "            image1_path = load_image(person1, int(idx1), lfw_funneled_dir)\n",
    "            image2_path = load_image(person2, int(idx2), lfw_funneled_dir)\n",
    "            \n",
    "            if image1_path is None or image2_path is None:\n",
    "                skipped += 1\n",
    "                continue  # Skip this pair if image loading fails\n",
    "            \n",
    "            embedding1 = get_embeddings(image1_path)\n",
    "            embedding2 = get_embeddings(image2_path)\n",
    "            \n",
    "            # Skip if either embedding is None (indicating an error)\n",
    "            if embedding1 is None or embedding2 is None:\n",
    "                skipped += 1\n",
    "                continue  # Skip this pair and move to the next one\n",
    "            \n",
    "            # Compute similarity\n",
    "            similarity = get_similarity(embedding1, embedding2)\n",
    "            predicted_label = 1 if similarity > threshold else 0\n",
    "            # print(f\"Pair: {image1} vs {image2}, Similarity: {similarity:.4f}, Predicted: {predicted_label}, Actual: {label}\")\n",
    "            predictions.append(predicted_label)\n",
    "        \n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            skipped += 1\n",
    "            continue  # Skip this pair if any image is not found\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pair {image1} vs {image2}: {e}\")\n",
    "            skipped += 1\n",
    "            continue  # Skip this pair on any other error\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels[:len(predictions)], predictions)  # Slice labels to match predictions\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%, Skipped pairs: {skipped}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0ffe9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_file = r\"LFW\\pairsDevTest.txt\"\n",
    "lfw_funneled_dir = r\"LFW\\lfw_funneled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "909f2780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detection Failed due to error.\n",
      "list index out of range\n",
      "Error processing LFW\\lfw_funneled\\Daisy_Fuentes\\Daisy_Fuentes_0003.jpg: pic should be PIL Image or ndarray. Got <class 'NoneType'>\n",
      "Face detection Failed due to error.\n",
      "list index out of range\n",
      "Error processing LFW\\lfw_funneled\\John_Stallworth\\John_Stallworth_0002.jpg: pic should be PIL Image or ndarray. Got <class 'NoneType'>\n",
      "Face detection Failed due to error.\n",
      "list index out of range\n",
      "Error processing LFW\\lfw_funneled\\Luis_Horna\\Luis_Horna_0002.jpg: pic should be PIL Image or ndarray. Got <class 'NoneType'>\n",
      "Face detection Failed due to error.\n",
      "list index out of range\n",
      "Error processing LFW\\lfw_funneled\\Tyler_Hamilton\\Tyler_Hamilton_0002.jpg: pic should be PIL Image or ndarray. Got <class 'NoneType'>\n",
      "Face detection Failed due to error.\n",
      "list index out of range\n",
      "Error processing LFW\\lfw_funneled\\Warren_Buffett\\Warren_Buffett_0001.jpg: pic should be PIL Image or ndarray. Got <class 'NoneType'>\n",
      "Face detection Failed due to error.\n",
      "list index out of range\n",
      "Error processing LFW\\lfw_funneled\\Brooke_Adams\\Brooke_Adams_0001.jpg: pic should be PIL Image or ndarray. Got <class 'NoneType'>\n",
      "Face detection Failed due to error.\n",
      "list index out of range\n",
      "Error processing LFW\\lfw_funneled\\Brooke_Adams\\Brooke_Adams_0001.jpg: pic should be PIL Image or ndarray. Got <class 'NoneType'>\n",
      "Face detection Failed due to error.\n",
      "list index out of range\n",
      "Error processing LFW\\lfw_funneled\\Ivan_Lee\\Ivan_Lee_0001.jpg: pic should be PIL Image or ndarray. Got <class 'NoneType'>\n",
      "Accuracy: 94.86%, Skipped pairs: 8\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_lfw(pair_file, lfw_funneled_dir, device, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8170a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model size: 94059852 bytes (89.70 MB)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Load TFLite model\n",
    "tflite_model_path = \"../IoT/Models/person_identification/facenetv2.tflite\"\n",
    "with open(tflite_model_path, \"rb\") as f:\n",
    "    tflite_model = f.read()\n",
    "\n",
    "# Check model size in bytes and MB\n",
    "tflite_model_size = os.path.getsize(tflite_model_path)\n",
    "print(f\"TFLite model size: {tflite_model_size} bytes ({tflite_model_size / (1024 * 1024):.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5a193c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model size: 6.75 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the size of the PyTorch model in MB\n",
    "torch_model_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f\"PyTorch model size: {torch_model_size / (1024 * 1024):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
